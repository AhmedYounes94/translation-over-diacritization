{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USE_DIACS = False\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "EMBEDDINGS_DIM = 100\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    diacritics_list = ''.join(['َ', 'ً', 'ُ', 'ٌ', 'ِ', 'ٍ', 'ّ', 'ْ'])\n",
    "    return text.translate(str.maketrans('', '', ''.join(diacritics_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_diacritics(text):\n",
    "    diacritics_list = ''.join(['َ', 'ً', 'ُ', 'ٌ', 'ِ', 'ٍ', 'ّ', 'ْ'])\n",
    "    diacritics = ''\n",
    "    for char in text:\n",
    "        if char in diacritics_list:\n",
    "            diacritics += char\n",
    "    if diacritics == '':\n",
    "        diacritics = '<none>'\n",
    "    return diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    ar_lines = open('data_dir/ar.bpe.train').read().strip().split('\\n')\n",
    "    for idx in range(len(ar_lines)):\n",
    "        ar_lines[idx] = '<start> ' + ar_lines[idx].strip() + ' <end>'\n",
    "    \n",
    "    en_lines = open('data_dir/en.bpe.train').read().strip().split('\\n')\n",
    "    for idx in range(len(en_lines)):\n",
    "        en_lines[idx] = '<start> ' + en_lines[idx].strip() + ' <end>'\n",
    "    \n",
    "    if USE_DIACS:            \n",
    "        ar_diac_lines = open('data_dir/ar-diac.bpe.train').read().strip().split('\\n')\n",
    "        for idx in range(len(ar_diac_lines)):\n",
    "            ar_diac_lines[idx] = ' '.join([extract_diacritics(token) for token in ar_diac_lines[idx].split()])\n",
    "            ar_diac_lines[idx] = '<start> ' + ar_diac_lines[idx] + ' <end>'\n",
    "\n",
    "        return ar_lines, ar_diac_lines, en_lines\n",
    "    \n",
    "    return ar_lines, en_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DIACS:\n",
    "    ar, ar_diac, en = create_dataset()\n",
    "else:\n",
    "    ar, en = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    if USE_DIACS:\n",
    "        ar_lang, ar_diac_lang, en_lang = create_dataset()\n",
    "    else:\n",
    "        ar_lang, en_lang = create_dataset()\n",
    "\n",
    "    ar_tensor, ar_lang_tokenizer = tokenize(ar_lang)\n",
    "    en_tensor, en_lang_tokenizer = tokenize(en_lang)\n",
    "    \n",
    "    if USE_DIACS:\n",
    "        ar_diac_tensor, ar_diac_lang_tokenizer = tokenize(ar_diac_lang)\n",
    "        return ar_tensor, ar_diac_tensor, en_tensor, ar_lang_tokenizer, ar_diac_lang_tokenizer, en_lang_tokenizer\n",
    "\n",
    "    return ar_tensor, en_tensor, ar_lang_tokenizer, en_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DIACS:\n",
    "    ar_tensor, ar_diac_tensor, en_tensor, ar_lang, ar_diac_lang, en_lang = load_dataset()\n",
    "    max_length_ar, max_length_ar_diac, max_length_en = max_length(ar_tensor), max_length(ar_diac_tensor), max_length(en_tensor)\n",
    "    print(max_length_ar, max_length_ar_diac, max_length_en)\n",
    "else:\n",
    "    ar_tensor, en_tensor, ar_lang, en_lang = load_dataset()\n",
    "    max_length_ar, max_length_en = max_length(ar_tensor), max_length(en_tensor)\n",
    "    print(max_length_ar, max_length_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"AR Language; index to word mapping\")\n",
    "convert(ar_lang, ar_tensor[0])\n",
    "print ()\n",
    "print (\"EN Language; index to word mapping\")\n",
    "convert(en_lang, en_tensor[0])\n",
    "if USE_DIACS:\n",
    "    print ()\n",
    "    print (\"AR DIAC Language; index to word mapping\")\n",
    "    convert(ar_diac_lang, ar_diac_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(ar_tensor)\n",
    "steps_per_epoch = len(ar_tensor) // BATCH_SIZE\n",
    "vocab_ar_size = len(ar_lang.word_index) + 1\n",
    "vocab_en_size = len(en_lang.word_index) + 1\n",
    "\n",
    "if USE_DIACS:\n",
    "    vocab_ar_diac_size = len(ar_diac_lang.word_index) + 1\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((ar_tensor, ar_diac_tensor, en_tensor)).shuffle(BUFFER_SIZE)\n",
    "else:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((ar_tensor, en_tensor)).shuffle(BUFFER_SIZE)\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DIACS:\n",
    "    example_ar_batch, example_ar_diac_batch, example_en_batch = next(iter(dataset))\n",
    "    print(example_ar_batch.shape, example_ar_diac_batch.shape, example_en_batch.shape)\n",
    "else:\n",
    "    example_ar_batch, example_en_batch = next(iter(dataset))\n",
    "    print(example_ar_batch.shape, example_en_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DIACS:\n",
    "    class Encoder(tf.keras.Model):\n",
    "        def __init__(self, ar_vocab_size, ar_diac_vocab_size, embedding_dim, units, batch_size):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.batch_size = batch_size\n",
    "            self.units = units\n",
    "            self.ar_embedding = tf.keras.layers.Embedding(ar_vocab_size,\n",
    "                                                          embedding_dim,\n",
    "                                                          embeddings_initializer='glorot_uniform')\n",
    "            self.ar_diac_embedding = tf.keras.layers.Embedding(ar_diac_vocab_size,\n",
    "                                                               embedding_dim,\n",
    "                                                               embeddings_initializer='glorot_uniform')\n",
    "            self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        def call(self, ar, ar_diac, hidden):\n",
    "            ar = self.ar_embedding(ar)\n",
    "            ar_diac = self.ar_diac_embedding(ar_diac)\n",
    "            output, state = self.gru(tf.keras.layers.concatenate([ar, ar_diac]), initial_state=hidden)\n",
    "            return output, state\n",
    "\n",
    "        def initialize_hidden_state(self):\n",
    "            return tf.zeros((self.batch_size, self.units))\n",
    "else:\n",
    "    class Encoder(tf.keras.Model):\n",
    "        def __init__(self, ar_vocab_size, embedding_dim, units, batch_size):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.batch_size = batch_size\n",
    "            self.units = units\n",
    "            self.ar_embedding = tf.keras.layers.Embedding(ar_vocab_size,\n",
    "                                                          embedding_dim,\n",
    "                                                          embeddings_initializer='glorot_uniform')\n",
    "            self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        def call(self, ar, hidden):\n",
    "            ar = self.ar_embedding(ar)\n",
    "            output, state = self.gru(ar, initial_state=hidden)\n",
    "            return output, state\n",
    "\n",
    "        def initialize_hidden_state(self):\n",
    "            return tf.zeros((self.batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DIACS:\n",
    "    encoder = Encoder(vocab_ar_size, vocab_ar_diac_size, EMBEDDINGS_DIM, UNITS, BATCH_SIZE)\n",
    "else:\n",
    "    encoder = Encoder(vocab_ar_size, EMBEDDINGS_DIM, UNITS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "if USE_DIACS:\n",
    "    sample_output, sample_hidden = encoder(example_ar_batch, example_ar_diac_batch, sample_hidden)\n",
    "else:\n",
    "    sample_output, sample_hidden = encoder(example_ar_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, en_vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(en_vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_en_size, EMBEDDINGS_DIM, UNITS, BATCH_SIZE)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)), sample_hidden, sample_output)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(sequences, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        if USE_DIACS:\n",
    "            enc_output, enc_hidden = encoder(sequences[0], sequences[1], enc_hidden)\n",
    "            en = sequences[2]\n",
    "        else:\n",
    "            enc_output, enc_hidden = encoder(sequences[0], enc_hidden)\n",
    "            en = sequences[1]\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([en_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, en.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(en[:, t], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(en[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(en.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, sequences) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(sequences, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sequences):\n",
    "    attention_plot = np.zeros((max_length_en, max_length_ar))\n",
    "    \n",
    "    for idx in range(len(sequences)):\n",
    "        sequences[idx] = '<start> ' + sequences[idx] + ' <end>'\n",
    "\n",
    "    if USE_DIACS:\n",
    "        ar = [ar_lang.word_index[i] for i in sequences[0].split(' ')]\n",
    "        ar = tf.keras.preprocessing.sequence.pad_sequences([ar],\n",
    "                                                           maxlen=max_length_ar,\n",
    "                                                           padding='post')\n",
    "        ar = tf.convert_to_tensor(ar)\n",
    "        \n",
    "        ar_diac = [ar_diac_lang.word_index[i] for i in sequences[1].split(' ')]\n",
    "        ar_diac = tf.keras.preprocessing.sequence.pad_sequences([ar_diac],\n",
    "                                                                maxlen=max_length_ar_diac,\n",
    "                                                                padding='post')\n",
    "        ar_diac = tf.convert_to_tensor(ar_diac)\n",
    "    else:\n",
    "        ar = [ar_lang.word_index[i] for i in sequences[0].split(' ')]\n",
    "        ar = tf.keras.preprocessing.sequence.pad_sequences([ar],\n",
    "                                                           maxlen=max_length_ar,\n",
    "                                                           padding='post')\n",
    "        ar = tf.convert_to_tensor(ar)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, UNITS))]\n",
    "    if USE_DIACS:\n",
    "        enc_out, enc_hidden = encoder(ar, ar_diac, hidden)\n",
    "    else:\n",
    "        enc_out, enc_hidden = encoder(ar, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([en_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_en):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += en_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if en_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sequences, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sequences, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sequences, plot_att=False):\n",
    "    result, sequences, attention_plot = evaluate(sequences)\n",
    "\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_att:\n",
    "        attention_plot = attention_plot[:len(result.split(' ')), :len(sequences[0].split(' '))]\n",
    "        plot_attention(attention_plot, sequences[0].split(' '), result.split(' '))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'data_dir/%s.bpe.test'\n",
    "if USE_DIACS:\n",
    "    test_file = test_file % 'ar-diac'\n",
    "else:\n",
    "    test_file = test_file % 'ar'\n",
    "\n",
    "with open(test_file, 'r') as file:\n",
    "    test_lines = file.readlines()\n",
    "\n",
    "result = list()\n",
    "for line in tqdm(test_lines):\n",
    "    line = line.strip()\n",
    "    \n",
    "    if USE_DIACS:\n",
    "        sequences = list()\n",
    "        sequences.append(remove_diacritics(line))\n",
    "        sequences.append(extract_diacritics(line))\n",
    "    else:\n",
    "        sequences = [line]\n",
    "    \n",
    "    result.append(translate(sequences))\n",
    "\n",
    "with open(test_file + '.predictions', 'w') as file:\n",
    "    file.write('\\n'.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
